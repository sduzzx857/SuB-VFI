# SuB-VFI
Official Implementation for "Subcellular behavior model enables highly precise temporal super-resolved live-cell imaging"

SuB-VFI is a frame interpolation method for subcellular images. SuB-VFI models the **behavior of subcellular particles** to enhance the accuracy of particle matching and improve interpolation outcomes. 
<video src="video/combined_video4.avi"></video>


## Dependencies and Installation
1. Clone Repo

   ```bash
   git clone https://github.com/sduzzx857/SuB-VFI.git
   ```

2. Create Conda Environment and Install Dependencies

   ```bash
   conda env create -f environment.yaml
   conda activate subvfi
   ```
## Pretrained Models

You can download the pretrained SuB-VFI model from [Google Driver](https://drive.google.com/drive/folders/1v3XXAmuJZOov1JaOiWMNhmJvKj8iM_Ee?usp=sharing)

The model path should be as follows:

```
./GMSF2D/pretrained/
    MICROTUBULE7high750_64_4.pth
    MICROTUBULE7low85_64_4.pth
    RECEPTOR7high910_64_4.pth
    RECEPTOR7low110_64_4.pth
    VESICLE7high1100_64_4.pth
    VESICLE7low135_64_4.pth
./pretrained/
    AMT/syn.pth
    CCR5.pth
    EB1.pth
    LYSOSOME.pth
    MICROTUBULE-high.pth
    MICROTUBULE-low.pth
    RECEPTOR-high.pth
    RECEPTOR-low.pth
    VESICLE-high.pth
    VESICLE-low.pth
```
* The `./GMSF2D/pretrained/` folder contains the pretrained model for the behavior model based on GMSF. 
* The `./pretrained/AMT/` folder contains the pretrained model for the frame synthesis module based on AMT. 
* The `./pretrained/` folder contains the pretrained model for the SuB-VFI model.

## Quick Demo

We have provided sample image sequences in the `./demos/images/` folder, along with the `.xml` files generated by tracking the particles in the image sequences using the Trackmate plugin.

Run `demo_2x.py` to double the temporal resolution of the sample image sequence and save the results in the ``./demos/results/`` folder.

 ```bash
 python demos/demo_2x.py
 ```

## Download Datasets
Please download preprocessed datasets from [the zenodo repository](https://zenodo.org/records/14043236). Or you can download the datasets from the original paper or repository and preprocess the dataset according to the following literatures:

* The simulated testing datasets can be downloaded from [the 2014 ISBI Particle Tracking Challenge](http://bioimageanalysis.org/track/). 
* The EB1 datasets can be downloaded from the paper [The dynamic behavior of the APC-binding protein EB1 on the distal ends of microtubules](https://www.cell.com/current-biology/fulltext/S0960-9822(00)00600-X?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS096098220000600X%3Fshowall%3Dtrue).  We used *Movie2* from the Supplementary data. Please name the video as `video.mov`, place it in the `./data_process/EB1/` folder, and run the following command to generate the training and testing datasets:

  ```bash
  python data_process/EB1/mov.py
  python data_process/EB1/resize.py
  ```

* The CCR5 datasets can be downloaded from the paper [Tracking receptor motions at the plasma membrane reveals distinct effects of ligands on CCR5 dynamics depending on its dimerization status](https://elifesciences.org/articles/76281). We used *Video4* in the Results section. Please name the video as `video.mp4`, place it in the `./data_process/CCR5/` folder, and run the following command to generate the training and testing datasets:

  ```bash
  python data_process/CCR5/mp4.py
  python data_process/CCR5/resize.py
  ```
* The Lysosome datasets can be downloaded from [Content-Aware Frame Interpolation Microscopy Datasets](https://zenodo.org/records/10076346). We used data from the `Zproject` folder within the compressed file `Source_Data_Lysosomes_z-proj_Fig_5.zip`, containing a total of 39 images. The first 20 images are used as the training set, while the remaining 19 images serve as the testing set.



The downloaded data path should be as follows:

```
data_root/
    SIMULATED/
        test/
            MICROTUBULE/
                MICROTUBULE snr 7 density low/
                MICROTUBULE snr 7 density high/
            RECEPTOR/
                RECEPTOR snr 7 density low/
                RECEPTOR snr 7 density high/
            VESICLE/
                VESICLE snr 7 density low/
                VESICLE snr 7 density high/
        train/
            MICROTUBULE/
            RECEPTOR/
            VESICLE/
    REAL/
        CCR5/
            test/
            train/
        EB1/
            test/
            train/
        LYSOSOME/
            test/
            train/
```

## Dataset Preprocess

1. Generate the training and test datasets for the behavior model based on GMSF, ensuring to modify `--root` to the path where you save your data:

    ```bash
    python GMSF2D/extract_particle.py --root data/
    ```

2. Generate the particle coordinates for the training and test datasets for the SuB-VFI model, also remembering to modify `--root` to your data path:

    ```bash
    python data_process/extract_particle.py --root data/
    ```

## Evaluation

1. Test the behavior model based on GMSF, ensuring to update `--root` in `test.sh` to your data path:

    ```shell
    ./GMSF2D/test.sh
    ```

2. Test the SuB-VFI model, also remembering to set `--root` to your data path:

    ```shell
    python benchmarks/Particle.py --root data/
    ```

## Training

1. Run the following commands for training the behavior model based on GMSF:
    ```shell
    ./GMSF2D/train.sh [CKP] [DATA] [DEN] [NPT] [STEP]
    ## e.g.
    ./GMSF2D/train.sh GMSF2D/checkpoints/MICROTUBULE-low MICROTUBULE low 85 20000
    ```

2. In the `.yaml` configuration files within the `cfgs/` folder, update the `data_root` parameter to the path where you save your data.

3. Before training, please first prepare the optical flows (which are used for supervision).

    We need to install `cupy` first before flow generation:

    ```shell
    conda install -c conda-forge cupy
    ```

    After installing `cupy`, we can generate optical flows by the following command:  

    ```shell
    ./flow_generation/gen_flow_particle.sh
    ```
4. After obtaining the optical flow of the training data, run the following commands for training:

    ```shell
    ./train.sh [PORT] [CFG]
    ## e.g.
    ./train.sh 14514 cfgs/CCR5.yaml
    ```


